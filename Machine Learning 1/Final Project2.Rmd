---
title: "Final Exam"
output: pdf_document
---

# Chapter 2 Question 10

## a)To begin, load in the Boston data set. The Boston data set is part of the MASS library in R.How many rows are in this data set? How many columns? What do the rows and columns represent?
```{r, echo = FALSE}
rm(list=ls())
library(MASS)
library(tidyverse)
attach(Boston)
dim(Boston)
??Boston
```
506 rows and 14 columns. The rows represent observations of the predictor values for each suburb in Boston. The columns represent each predictor variables for housing values in the suburbs of Boston.

## b)Make some pairwise scatterplots of the predictors (columns) in this data set. Describe your findings.
```{r, echo = FALSE}
## to create plots
pairs(Boston)
## create pairs with only 4 variables
pairs(Boston[,1:4])
# pairs of just crim and medv
```

When I ran all the pairs plot for all the variables it was not possible to discern patterns due to the crowding. In order to mitigate this I used just the first 4 variables. From the first 4 variables it seems there is a slightly negative relationship between zn and indus. There is a also a negative relationship between crim and medv.

## c) Are any of the predictors associated with per capita crime rate? If so, explain the relationship.
Correlations between variables
```{r,echo = FALSE}
## correlation numbers to see correlation relationship
cor(Boston$crim,Boston$zn)
cor(Boston$crim,Boston$indus)
cor(Boston$crim,Boston$chas)
cor(Boston$crim,Boston$nox)
cor(Boston$crim,Boston$rm)
cor(Boston$crim,Boston$age)
cor(Boston$crim,Boston$dis)
cor(Boston$crim,Boston$rad)
cor(Boston$crim,Boston$tax)
cor(Boston$crim,Boston$ptratio)
cor(Boston$crim,Boston$black)
cor(Boston$crim,Boston$lstat)
```

The proportion of non-retail business acres per town, nitrogen oxide concentrations, index of accessibility to radial highways, and full-value property tax rate seem to be strong predictors for crime rate based on the correlations levels. However, more analysis would be necessary to say definitively.

## d) Do any of the suburbs of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of each predictor
```{r,echo = FALSE}
hist(Boston$crim)
summary(Boston$crim)
hist(Boston$tax)
summary(Boston$tax)
hist(Boston$ptratio)
summary(Boston$ptratio)

```
The histogram of Boston suburbs crime rate shows that there is a high frequency of suburbs with a crime rate between 0 and 10 percent. However there are around 20 suburbs that have higher crimes rates and are considered outliers in this data. The range of the crimes rates in the suburbs is 0.00632% to 88.97%. 
The histogram of Boston suburbs full property tax rate is more normally distributed then the crime rates. The highest frequency of tax rates is centered at 650 percent with over 120 suburbs. The outliers are present in the middle of the histogram with less than 20 suburbs having a tax rate between 450 and 500 percent. The range of the tax rates is 187.0 percent to 711.0 percent.
The histogram of Boston suburbs pupil to teacher ratio is more normally distributed then the crime histogram with the highest frequency of suburbs showing a pupil teacher ratio between 20 and 21 made up of over 150 suburbs. The suburbs showing a pupil teacher ratio of under 13 and over 21 are outliers. The range of the pupil to teacher ratio is 12.60 to 22. 

## e)How many of the suburbs in this data set bound the Charles river?
```{r, echo = FALSE}
charlesRiver <- subset(Boston,chas==1)
nrow(charlesRiver)
```
35 suburbs

## f)What is the median pupil-teacher ratio among the towns in this data set?
```{r, echo=FALSE}
median(Boston$ptratio)

```
19.05 is the median pupil-teacher ratio among the towns in this data set

## g) Which suburb of Boston has lowest median value of owneroccupied homes? What are the values of the other predictors for that suburb, and how do those values compare to the overall ranges for those predictors? Comment on your findings.

```{r, echo = FALSE}
selection <-Boston[order(Boston$medv),]
selection[1,]
summary(Boston)
```

Suburb 399 had the lowest median value of owner occupied homes. The other predictors for this suburb include indus at 18.1, the range for indus in the Boston set is 27.74-0.46 with the mean at 11.14 meaning this suburb is above average for indus. Chas is 0 meaning the suburb is not on the river. Nox is 0.693, the range for nox is 0.8710-0.3850 and the mean is 0.5547 meaning this suburb is above the average for nox. Rm is 5.453, the range for rm in the Boston set is 8.780-3.561 and the mean is 6.285 meaning this suburb is below the average for rm.
Age is 100, the range for age in the Boston set is 100.00-2.90 and the mean is 68.57 meaning this suburb is at the highest end of the range and above the mean for age. Dis is 1.4896,the range for dis is 12.127-1.130 and the mean is 3.795 meaning this suburb is below the average for dis. Rad is 24, the range is 24-1 and the mean is 9.549 meaning this suburb is above the average and at the highest end of the range for rad.Tax is 666, the range for tax in the Boston set is 711-187 and the mean for tax is 408.2 meaning that tax for this suburb is above the average.Ptratio is 20.2,the range for ptratio is 22-12.60 and the mean is 18.46 meaning the ptratio for this suburb is above the average. Black 396.9, the range for black in the Boston set is 396.90-0.32 and the mean is 356.67 meaning this suburb is above average and at the highest end of the range for black.Lstat is 30.59,the range is 37.97-1.73 and the mean is 12.65 meaning this suburb is above the average for lstat.Medv is 5, the range for medv in the Boston set is 50-5 and the mean is 22.53 meaning medv for this suburb is below average and on the lowest extreme of the range.

## h) In this data set, how many of the suburbs average more than seven rooms per dwelling? More than eight rooms per dwelling? Comment on the suburbs that average more than eight rooms per dwelling.
```{r,echo= FALSE}
roomsOver7 <- subset(Boston,rm>7)
nrow(roomsOver7)
roomsOver8 <- subset(Boston,rm>8)
nrow(roomsOver8)
```
64 suburbs average more than seven rooms per dwelling and 13 suburbs average more than eight rooms per dwelling. The 13 suburbs that average more than eight rooms per dwelling also have a higher average age at 71.54 then the overall average of the age of all the suburbs included in the data set.This could indicate that these homes are multi-generational homes that require more rooms and have older adults dwelling there. The average tax in the 13 suburbs at 325.1 is lower than the average tax of the overall data. 

# Chapter 3 Question 15

## This problem involves the Boston data set, which we saw in the lab for this chapter. We will now try to predict per capita crime rate using the other variables in this data set. In other words, per capita crime rate is the response, and the other variables are the predictors

##  a) For each predictor, fit a simple linear regression model to predict the response. Describe your results. In which of the models is there a statistically significant association between the predictor and the response? Create some plots to back up your assertions.

```{r,echo=FALSE,warning=FALSE}
rm(list=ls())
library(MASS)
attach(Boston)
??Boston
## linear models
lmfitage=lm(crim~age)
lmfitblack=lm(crim~black)
lmfitchas=lm(crim~chas)
lmfitdis=lm(crim~dis)
lmfitindus=lm(crim~indus)
lmfitlstat=lm(crim~lstat)
lmfitmedv=lm(crim~medv)
lmfitnox=lm(crim~nox)
lmfitptratio=lm(crim~ptratio)
lmfitrad=lm(crim~rad)
lmfitrm=lm(crim~rm)
lmfittax=lm(crim~tax)
lmfitzn=lm(crim~zn)
##summary of linear models
summary(lmfitage)
summary(lmfitblack)
summary(lmfitchas)
summary(lmfitdis)
summary(lmfitindus)
summary(lmfitlstat)
summary(lmfitmedv)
summary(lmfitnox)
summary(lmfitptratio)
summary(lmfitrad)
summary(lmfitrm)
summary(lmfittax)
summary(lmfitzn)
## plotting relationship between variables
plot(crim ~ tax)
plot(crim ~ medv)
plot(crim ~lstat)
plot(crim ~ black)
plot(crim ~ rad)
```

Every predictor besides chas is a statistically significant predictor based on the p-value being less than 0.05 which means we can reject the null hypothesis and conclude that the predictor does have an effect on crime. The predictors with the highest t-values and lowest p-values include tax, medv, lstat, rad, and black. These predictors have the highest statistical significance of being able to accurately predict crime in a single linear regression model. Zn had a low t-value and a higher p-value then the other statistically significant predictors but the value is still considered significant. 

The crim/tax plot shows a slight positive sloping trend between the tax rate and the crime rate which supports the positive prediction coefficient produced by the linear model. The crim/medv plot shows a slight negative sloping trend between the median home value and the crime rate. This supports the negative prediction coefficient produced by the linear model. The crim/lstat plot shows shows a slight positive slope between the lower status of the population and the crime rate. This supports the positive prediction coefficient produced by the linear model. The crim/black plot is more scattered but may show a slight positive slope between the black variable and the crime rate response. Finally, the crim/rad plot does not show less of a distinct relationship between crime and radical highways.


## b) Fit a multiple regression model to predict the response using all of the predictors. Describe your results. For which predictors can we reject the null hypothesis?

```{r, echo=FALSE,warning=FALSE}
## linear model with all predictors
lm.fit=lm(crim~.,data=Boston)
## summary of linear model
summary(lm.fit)
```

The results show that we can reject the null hypothesis for zn, dis, rad, black, and medv. We can reject the null hypothesis and state these predictors are statistically significant in predicting the crime rate because they have a p-value of less then 0.05. Rad and dis have the greatest statistical significance in predicting crime rate holding every other variable constant. The model shows that for every 1 percent increase in the index of accessibility to radical highways, the per capita crime rate increases by 0.58 holding all other variables constant.
Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.


## c) How do your results from (a) compare to your results from (b)? Create a plot displaying the univariate regression coefficients from (a) on the x-axis, and the multiple regression coefficients from (b) on the y-axis. That is, each predictor is displayed as a single point in the plot. Its coefficient in a simple linear regression model is shown on the x-axis, and its coefficient estimate in the multiple linear regression model is shown on the y-axis.
```{r,echo=FALSE,warning=FALSE}
multipleModel <- lm.fit
multipleModelCoef <- multipleModel$coefficients[2:14]
singleModel <- c(lmfitage$coefficients[2],lmfitblack$coefficients[2],lmfitchas$coefficients[2],lmfitdis$coefficients[2],lmfitindus$coefficients[2],lmfitlstat$coefficients[2],lmfitmedv$coefficients[2],lmfitnox$coefficients[2],lmfitptratio$coefficients[2],lmfitrad$coefficients[2],lmfitrm$coefficients[2],lmfittax$coefficients[2],lmfitzn$coefficients[2])
plot(singleModel,multipleModelCoef,xlab="single linear regression",ylab="multiple linear regression")
```
The graph results show that the single linear regression coefficients are slightly larger than the multiple linear regression coefficients for all the variables except nox. Nox has a single linear regression coefficient of over 30 and  multiple linear regression coefficient of slightly over 1.

The coefficient results from a to b differ because once all the variables are used in the model, it changes the p-values and makes it more clear which variables are actually statistically significant for predicting crim. The only variables that was not statistically significant in univariate regression was chas. However, in the multi linear regression model we can see that indus, chas, nox, rm ,age, tax, ptratio, and lstat are not statistically significant. This means that these predictors having an initial relation with crim is likely due to chance and these predictors are not appropriate for prediction.

## d) Is there evidence of non-linear association between any of the predictors and the response? To answer this question, for each predictor X, fit a model of the form

```{r,echo=FALSE,warning=FALSE}
#non-linear transformation equations
lmfit2age = lm(crim ~ age + I(age^2) + I(age^3))
summary(lmfit2age)
lmfit2black = lm(crim ~ black + I(black^2) + I(black^3))
summary(lmfit2black)
lmfit2chas = lm(crim ~ chas + I(chas^2) + I(chas^3))
summary(lmfit2chas)
lmfit2dis = lm(crim ~ dis + I(dis^2) + I(dis^3))
summary(lmfit2dis)
lmfit2indus = lm(crim ~ indus + I(indus^2) + I(indus^3))
summary(lmfit2indus)
lmfit2lstat = lm(crim ~ lstat + I(lstat^2) + I(lstat^3))
summary(lmfit2lstat)
lmfit2medv = lm(crim ~ medv + I(medv^2) + I(medv^3))
summary(lmfit2medv)
lmfit2nox = lm(crim ~ nox + I(nox^2) + I(nox^3))
summary(lmfit2nox)
lmfit2ptratio = lm(crim ~ ptratio + I(ptratio^2) + I(ptratio^3))
summary(lmfit2ptratio)
lmfit2rad = lm(crim ~ rad + I(rad^2) + I(rad^3))
summary(lmfit2rad)
lmfit2rm = lm(crim ~ rm + I(rm^2) + I(rm^3))
summary(lmfit2rm)
lmfit2tax = lm(crim ~ tax + I(tax^2) + I(tax^3))
summary(lmfit2tax)
lmfit2zn = lm(crim ~ zn + I(zn^2) + I(zn^3))
summary(lmfit2zn)
```

There is evidence of a non-linear association between crime and the following predictors: dis, indus, medv, nox, ptratio,and age. All of these predictors have a statistically significant t-value for the square and cubed factor because the p-value for the predictor is less than 0.05 allowing us to rejected the null hypothesis and confirm that there is evidence of a non-linear association between these predictors and the crime rate.

# Chapter 6 Question 9 

## In this exercise, we will predict the number of applications received using the other variables in the College data set

## a) Split the data set into a training set and a test set.
```{r setup, echo=FALSE,warning=FALSE,message=FALSE}
rm(list=ls())
set.seed(1)
library(glmnet)
library(ISLR)
attach(College)
x=model.matrix(Apps~.,College )[,-1]
y=College$Apps
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
train=sample(1:nrow(x),nrow(x)/2,rep=FALSE)
## Create training and test set
test=(-train)
y.test=y[test]
print("train")
dim(x[train,])
print("test")
dim(x[test,])
```

## b) Fit a linear model using least squares on the training set, and report the test error obtained.
```{r question1,echo=FALSE}
# linear model creation
linearmodel<-lm(Apps~.,data=College,subset=train)
summary(linearmodel)
yhat=predict(linearmodel,newdata=College[-train,])
# calculate MSE
mean((yhat-y.test)^2)
```
MSE is 1135758

## c) Fit a ridge regression model on the training set, with lambda chosen by cross-validation. Report the test error obtained.
```{r question2,echo=FALSE}
cv.out=cv.glmnet(x[train ,],y[train],alpha=0)
plot(cv.out)
# bestlam is the minimum chosen by cross-validation
bestlam=cv.out$lambda.min
bestlam
ridge.pred=predict(ridge.mod,s=bestlam,newx = x[test, ])
mean((ridge.pred-y.test)^2)


```
Best lam is 405
MSE is 906029.4. This MSE is much lower then the MSE resulting from the linear regression model.

## d)Fit a lasso model on the training set, with lambda chosen by crossvalidation. Report the test error obtained, along with the number of non-zero coefficient estimates
```{r question3,echo=FALSE,warning=FALSE}
set.seed(123)
lasso.mod=glmnet(x[train ,],y[ train],alpha=1,lambda=grid)
plot(lasso.mod)
cv.out=cv.glmnet(x[train ,],y[ train],alpha=1)
plot(cv.out)
# lambda chosen by cross validation 
bestlam=cv.out$lambda.min
# fitting lasso model on training set
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test, ])
mean((lasso.pred-y.test)^2)
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)
lasso.coef


```
MSE is 1065095, PrivateYes,Accept,Enroll,Top10Perc,Top25Perc,P.Undergrad,Outstate,Room.Board,Personal,PhD,Terminal,S.F.Ratio,perc.alumni,Expend,and Grad.Rate are none zero estimates. 15 total non-zero coefficients

## e) Fit a PCR model on the training set, with M chosen by crossvalidation. Report the test error obtained, along with the value of M selected by  cross-validation.
```{r question4,echo=FALSE}
library(pls)
set.seed(2)
pcr.fit=pcr(Apps~.,data=College,scale=TRUE,validation="CV")
summary(pcr.fit)
validationplot(pcr.fit,val.type="MSEP")
pcr.fit=pcr(Apps~.,data=College,subset=train,scale=TRUE,validation="CV")
# m=17, chosen due to smallest CV and adjCV from summary of pcr.fit
pcr.pred=predict(pcr.fit,x[test, ],ncomp=17)
mean((pcr.pred-y.test)^2)

```
MSE is 1135758, the m value is 17

## f)Fit a PLS model on the training set, with M chosen by crossvalidation. Report the test error obtained, along with the value of M selected by cross-validation.
```{r question5,echo=FALSE}
set.seed(1)
pls.fit=plsr(Apps~.,data=College,subset=train,scale=TRUE,validation="CV")
summary(pls.fit)
validationplot(pls.fit,val.type="MSEP")
# m=15, chosen due to smallest CV and adjCV value in summary of pls.fit
pls.pred=predict(pls.fit,x[test ,],ncomp=15)
mean((pls.pred-y.test)^2)
```
MSE is 1135806, m is 15

## g) Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these five approaches?
We can most accurately predict the number of college applications using the ridge regression model. There is a difference between the test error resulting from the PLS, Lasso, and ridge regression model.However, the error obtained from the linear regression model and PCR model are the same. The MSE from the PLS model is the largest at 1135806, even when the lambda was chosen by cross validation. The test errors of the five approaches can be ordered as follows with the lowest MSE first: Ridge Regression,lasso,PCR,linear regression, and PLS.

# Chapter 6 Question 11

## We will now try to predict per capita crime rate in the Boston data set.

## a) Try out some of the regression methods explored in this chapter, such as best subset selection, the lasso, ridge regression, and PCR. Present and discuss results for the approaches that you consider.

### Best Subset Selection
```{r, echo=FALSE,warning=FALSE}
### Best Subset Selection
rm(list=ls())
set.seed(1)
library(ISLR)
library(MASS)
library(leaps)
attach(Boston)
regfit.full=regsubsets(crim~.,Boston)
summary(regfit.full)
regfit.full=regsubsets(crim~.,data=Boston ,nvmax=13)
reg.summary=summary(regfit.full)
reg.summary$rsq
names(reg.summary)
par(mfrow=c(2,2))
plot(reg.summary$rss ,xlab="Number of Variables ",ylab="RSS",type="l")
which.max(reg.summary$rss)
points(1,reg.summary$rss[1],col="red",cex=2,pch =20)
plot(reg.summary$adjr2,xlab="Number of Variables ",ylab="Adjusted RSq",type="l")
which.max(reg.summary$adjr2)
points(9,reg.summary$adjr2[9], col="red",cex=2,pch =20)
plot(reg.summary$cp ,xlab="Number of Variables ",ylab="Cp",type="l")
which.min(reg.summary$cp)
points(8,reg.summary$cp [8], col ="red",cex=2,pch =20)
plot(reg.summary$bic ,xlab="Number of Variables ",ylab="BIC",type="l")
which.min(reg.summary$bic )
points(3,reg.summary$bic [3],col="red",cex=2,pch =20)

```

```{r,echo=FALSE,warning=FALSE}
plot(regfit.full ,scale="r2")
plot(regfit.full ,scale="adjr2")
plot(regfit.full ,scale="Cp")
plot(regfit.full ,scale="bic")
coef(regfit.full ,4)
```
The Best subset selection model shows that the model with the lowest BIC is a model that only includes rad,black,and lstat.The variables produce a model with BIC -260.

### Lasso 
```{r,echo=FALSE,warning=FALSE}
### Lasso
rm(list=ls())
set.seed(1)
library(MASS)
library(glmnet)
x=model.matrix(crim~.,Boston )[,-1]
y=Boston$crim
train=sample (1: nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]
grid=10^seq(10,-2, length =100)
lasso.mod=glmnet(x[train ,],y[ train],alpha=1, lambda =grid)
plot(lasso.mod)
cv.out=cv.glmnet(x[train ,],y[ train],alpha=1)
plot(cv.out)
bestlam =cv.out$lambda.min
lasso.pred=predict(lasso.mod ,s=bestlam ,newx=x[test ,])
mean((lasso.pred -y.test)^2)
out=glmnet(x,y,alpha=1, lambda=grid)
lasso.coef=predict(out,type="coefficients",s= bestlam)
lasso.coef
```
MSE is 40.8975, best lambda is 0.06,all the coefficients are non zero except for age and tax. The lasso model with lambda chosen by cross validation contains only 11 variables.

### Ridge Regression
```{r,echo=FALSE,warning=FALSE}
### Ridge Regression
rm(list=ls())
set.seed(1)
library(MASS)
library(glmnet)
x=model.matrix(crim~.,Boston )[,-1]
y=Boston$crim
grid=10^seq(10,-2, length =100)
ridge.mod=glmnet (x,y,alpha=0, lambda=grid)
train=sample (1: nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]
cv.out=cv.glmnet(x[train ,],y[ train],alpha=0)
plot(cv.out)
bestlam =cv.out$lambda.min
bestlam
ridge.pred=predict (ridge.mod ,s=bestlam ,newx=x[test ,])
mean((ridge.pred -y.test)^2)
out=glmnet(x,y,alpha=0)
predict(out,type="coefficients",s= bestlam)
```
The MSE is 38.01174 and the lambda chosen by cross validation is 0.59

### PCR
```{r,echo=FALSE,warning=FALSE}
### PCR
rm(list=ls())
set.seed(1)
library(MASS)
library(pls)
x=model.matrix(crim~.,Boston )[,-1]
y=Boston$crim
train=sample (1: nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]
pcr.fit=pcr(crim~., data=Boston , scale=TRUE ,validation ="CV")
summary (pcr.fit)
validationplot(pcr.fit ,val.type="MSEP")
pcr.fit=pcr(crim~., data=Boston , subset=train ,scale=TRUE ,validation ="CV")
validationplot(pcr.fit,val.type="MSEP")
# m=13, chosen due to smallest CV and adjCV value in summary of pcr.fit
pcr.pred=predict(pcr.fit ,x[test ,],ncomp =13)
mean((pcr.pred -y.test)^2)
```
MSE is 41.54639, m is 13

## b) Propose a model (or set of models) that seem to perform well on this data set, and justify your answer. Make sure that you are evaluating model performance using validation set error, crossvalidation, or some other reasonable alternative, as opposed to using training error
The ridge regression model had the best performance with the smallest MSE at 38.01174 using a lamda of 0.59 chosen from cross validation.The smallest test error makes the ridge regression model the most ideal.This constraint method allows you to work with all the predictors while still keeping a restraint on the coefficient size. The small lamda size also helps to avoid bias and by using cross validation to chose the lambda size, the model chooses lambda by comparing the out of sample performance.

## c)Does your chosen model involve all of the features in the data set? Why or why not?
My chosen model uses all the variables because ridge regression does not perform variable selection

# Chapter 4 Question 10

## This question should be answered using the Weekly data set, which is part of the ISLR package. This data is similar in nature to the Smarket data from this chapter’s lab, except that it contains 1, 089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010.

## a) Produce some numerical and graphical summaries of the Weekly data. Do there appear to be any patterns
```{r,echo=FALSE,warning=FALSE}
rm(list=ls())
library(ISLR)
attach(Weekly)
## Look at summary
summary(Weekly)
## Plot pairs graphs to look for correlation
pairs(Weekly)
## Look at correlation numbers between variables
print("Correlations")
cor(subset(Weekly, select = -Direction))

```
There does not appear to be any real patterns from the plots
Volume and Year are the only predictors that seem to show a pattern of a relationship.The plot of Volume and Year shows a positive correlation pattern and the correlation between Volume and Year is very close to 1 at 0.8 showing there is some correlation between these variables. None of the other variables show a strong discernible pattern. 

## b) Use the full data set to perform a logistic regression with Direction as the response and the five lag variables plus Volume as predictors. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?
```{r,echo=FALSE,warning=FALSE}
## Logistic regression using all five lag variables as predictors 
glm.fits=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,data=Weekly,family=binomial)
summary (glm.fits)

```
Lag 2 is the only statistically significant variable with a p-value below 0.05 and a high t-value. This means that the percentage return for 2 weeks previous has a 0.05844 impact on the direction of the of the market on a given week.

## c) Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression
```{r,echo=FALSE,warning=FALSE}
## Equation for confusion matrix using logistic regression from b)
glm.probs=predict(glm.fits,Weekly,type="response")
contrasts(Direction)
## Use 1089 observations
glm.pred=rep("Down" ,1089)
glm.pred[glm.probs >.5]=" Up"
## Print table
table(glm.pred,Direction)


```
The model correctly predicted that the market would go up on 557 weeks and correctly predicted it would go down on 54 weeks. Logistic regression correctly predicted the outcome 56% of the time. The confusion matrix also tells me that for this set of data, the logistic regression model is more likely to make a mistake predicting that the direction is going up when it actually is going down compared to fewer mistakes made when predicting that the market is going down when it actually is going up.The training error rate is 44%.

## d) Now fit the logistic regression model using a training data period from 1990 to 2008, with Lag2 as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010)
```{r,echo=FALSE,warning=FALSE}
## Create train data for years prior to 2009
train=(Year <2009)
## Create test data with the years after 2008
Weekly.2009= Weekly [!train ,]
## Dimensions of test
dim(Weekly.2009)
## Test data for Direction
Direction.2009 =Direction[!train]
## Logistic regression model
glm.fits=glm(Direction ~Lag2,data=Weekly,family=binomial,subset=train)
summary(glm.fits)
## Create confusion matrix
glm.probs=predict(glm.fits,Weekly.2009, type="response")
glm.pred=rep("Down",104)
glm.pred[glm.probs >.5]=" Up"
## Print confusion matrix
table(glm.pred,Direction.2009)
```
The model correctly predicted that the market would go up on 56 weeks and correctly predicted that it would go down on 9 weeks. Meaning that the model correctly predicted the result 62.5% of the time.The model has a test error rate of 37.5%. This is an improvement over the previous model without using held out data.

## g) Repeat (d) using KNN with K = 1.
```{r,echo=FALSE,warning=FALSE}
library(class)
set.seed(1)
## Create training set, test set, and validation
train.data = Weekly[Weekly$Year<2009,]
test.data = Weekly[Weekly$Year>2008,]
train.X = cbind(train.data$Lag2)
test.X = cbind(test.data$Lag2)
train.Y = cbind(train.data$Direction)
## KNN model
knn.pred=knn(train.X,test.X,train.Y,k=1)
contrasts(Direction)
## Print confusion matrix
table(knn.pred,Direction.2009)

```
The model accurately predicts that the market will go up 31 times and the model accurately predicts that the market will go down 21 times. The model accurately predicts the direction 50% of the time. The test error rate is also 50%.

## h) Which of these methods appears to provide the best results on this data?
The logistic regression model using a training data period from 1990 to 2008 with Lag2 as the only predictor provided the best predictive results for this data due to the low test error rate.

## i) Experiment with different combinations of predictors, including possible transformations and interactions, for each of the methods. Report the variables, method, and associated confusion matrix that appears to provide the best results on the held out data. Note that you should also experiment with values for K in the KNN classifier.
```{r,echo=FALSE,warning=FALSE}
print("Logistic Regression Model using Lag1, Lag2, and Lag3")
## Create training data for years prior to 2009
train=(Year <2009)
## Create test data
Weekly.2009= Weekly [!train ,]
dim(Weekly.2009)
## Create direction validaiton data
Direction.2009 =Direction[!train]
## Logistic regression model
glm.fits=glm(Direction ~Lag1+Lag2+Lag3,data=Weekly,family=binomial,subset=train)
summary(glm.fits)
## Create confusion matrix
glm.probs=predict(glm.fits,Weekly.2009, type="response")
glm.pred=rep("Down",104)
glm.pred[glm.probs >.5]=" Up"
## Print matrix
table(glm.pred,Direction.2009)
## 57.6

print("Logistic Regression Model using Lag1,Lag2,Lag3, and Lag4")
glm.fits=glm(Direction ~Lag1+Lag2+Lag3+Lag4 ,data=Weekly,family=binomial,subset=train)
summary(glm.fits)
## Create confusion matrix
glm.probs=predict(glm.fits,Weekly.2009, type="response")
glm.pred=rep("Down",104)
glm.pred[glm.probs >.5]=" Up"
## Print matrix
table(glm.pred,Direction.2009)

## Accuracy = 58.6

print("Logistic Regression Model using Lag1, Lag2, and Volume")
glm.fits=glm(Direction ~Lag1+Lag2+Volume ,data=Weekly,family=binomial,subset=train)
summary(glm.fits)
## Create confusion matrix
glm.probs=predict(glm.fits,Weekly.2009, type="response")
glm.pred=rep("Down",104)
glm.pred[glm.probs >.5]=" Up"
## Print matrix
table(glm.pred,Direction.2009)
##Accuracy = 52.8

print("KNN classifier with K=5 and using Lag2")
library(class)
set.seed(1)
## Create training,test, and validation set
train.data = Weekly[Weekly$Year<2009,]
test.data = Weekly[Weekly$Year>2008,]
train.X = cbind(train.data$Lag2)
test.X = cbind(test.data$Lag2)
train.Y = cbind(train.data$Direction)
## KNN model with k=5
knn.pred=knn(train.X,test.X,train.Y,k=5)
contrasts(Direction)
## Print matrix
table(knn.pred,Direction.2009)
## Accuracy =53.8

print("KNN classifier with K=2 and using Lag2")
## Create training,test, and validation set
train.data = Weekly[Weekly$Year<2009,]
test.data = Weekly[Weekly$Year>2008,]
train.X = cbind(train.data$Lag2)
test.X = cbind(test.data$Lag2)
train.Y = cbind(train.data$Direction)
## KNN model with k=2
knn.pred=knn(train.X,test.X,train.Y,k=2)
contrasts(Direction)
## Print confusion matrix
table(knn.pred,Direction.2009)
## Accuracy = 53.8
```

The best model that I created was a logistic regression model using Lag1,Lag2,Lag3,and Lag4 to predict Direction. The confusion matrix is below:

```{r,echo=FALSE}
glm.fits=glm(Direction ~Lag1+Lag2+Lag3+Lag4 ,data=Weekly,family=binomial,subset=train)
summary(glm.fits)
## Create confusion matrix
glm.probs=predict(glm.fits,Weekly.2009, type="response")
glm.pred=rep("Down",104)
glm.pred[glm.probs >.5]=" Up"
## Print matrix
table(glm.pred,Direction.2009)
```

The matrix shows that the model accurately predicted the direction going up 53 times on the test data and accurately predicted the direction doing down 8 times. It is seen with all the models that models created for this data seem to accurately predict the direction going up then going down. However, this particular model produced the highest accuracy, correctly predicting the direction 58.6% of the time. The test error is 41.4%.I did experiment with values of K=2 and K=5 and both models had a lower prediction accuracy then the logistic regression model with Lag1, Lag2, Lag3,and Lag4.

# Chapter 8 Question 8

## In the lab, a classification tree was applied to the Carseats data set after converting Sales into a qualitative response variable. Now we will seek to predict Sales using regression trees and related approaches,treating the response as a quantitative variable.

## (a) Split the data set into a training set and a test set.
```{r, echo = FALSE,warning=FALSE}
rm(list=ls())
library(ISLR)
library(tree)
set.seed(1)
??Carseats
# split the data
train = sample (1:nrow(Carseats), nrow(Carseats)/2)
Carseats.test=Carseats [-train ,]
print("train")
dim(Carseats [train ,])
print("test")
dim(Carseats [-train ,])

```
## b) Fit a regression tree to the training set. Plot the tree, and interpret the results. What test MSE do you obtain?
```{r, echo=FALSE}
# fit the tree to the training set
tree.carseats=tree(Sales ~.,data=Carseats,subset=train)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats,pretty=0)
# predict yhat
yhat=predict(tree.carseats,newdata=Carseats[-train,])
carseats.test=Carseats[-train,"Sales"]
# calculate MSE
mean((yhat-carseats.test)^2)
```
MSE equals 4.922039
The tree is very messy and does not seem to be an effective visual model. The tree would benefit from pruning in order to increase visibility and focus on the variables that are most important in predicting car seat sales.

## c) Use cross-validation in order to determine the optimal level of tree complexity. Does pruning the tree improve the test MSE?
```{r,echo=FALSE,warning=FALSE}
set.seed(4)
cv.carseats=cv.tree(tree.carseats)
plot(cv.carseats$size ,cv.carseats$dev,type="b")
# cross validation plot shows that 7 is the best for size of tree
print("Cross validation plot shows that 7 is best m")
prune.carseats=prune.tree(tree.carseats,best = 7)
plot(prune.carseats)
abline(0,1)
text(prune.carseats,pretty=0)
yhat=predict(prune.carseats,pruneData=Carseats[-train,])
mean((yhat-carseats.test)^2)

```
MSE is 11.68142
Pruning the tree does not improve the MSE because it is higher after the pruning then it was with just the regular regression fitting. This suggests that a larger tree is a better fit for this model.

## d) Use the bagging approach in order to analyze this data. What test MSE do you obtain? Use the importance() function to determine which variables are most important
```{r,echo=FALSE,warning=FALSE}
library(randomForest)
set.seed(1)
# bagging method
bag.carseats= randomForest(Sales ~.,data=Carseats,subset=train,mtry=10,importance =TRUE)
bag.carseats
yhat.bag=predict(bag.carseats,newdata=Carseats[-train,])
plot(yhat.bag,carseats.test)
abline(0,1)
mean((yhat.bag-carseats.test)^2)
# importance function to determine which variables are most important
importance(bag.carseats)
# plot the importance variables 
varImpPlot(bag.carseats)
```
MSE is 2.605253
Price,Shelf location, and competitor's price are the most important variables in determining the if a consumer will purchase a car seat.

## e) Use random forests to analyze this data. What test MSE do you obtain? Use the importance() function to determine which variables are most important. Describe the effect of m, the number of variables considered at each split, on the error rate obtained
```{r,echo=FALSE,warning=FALSE}
set.seed(1)
print("random forest method with m = 3")
randomForestCarseats= randomForest(Sales ~.,data=Carseats , subset=train ,mtry=3, importance=TRUE)
yhat.rf = predict(randomForestCarseats ,newdata=Carseats[- train ,])
# calculate MSE
mean((yhat.rf-carseats.test)^2)
# get importance of variables and plot
importance(randomForestCarseats)
varImpPlot(randomForestCarseats)

#MSE is 2.960559

set.seed(1)
print("random forest method with m = 4")
randomForestCarseats= randomForest(Sales ~.,data=Carseats , subset=train ,mtry=4, importance=TRUE)
yhat.rf = predict(randomForestCarseats ,newdata=Carseats[- train ,])
# calculate MSE
mean((yhat.rf-carseats.test)^2)
# get importance of variables and plot
importance(randomForestCarseats)
varImpPlot(randomForestCarseats)

# MSE is 2.787584

set.seed(1)
print("random forest method with m = 5")
randomForestCarseats= randomForest(Sales ~.,data=Carseats , subset=train ,mtry=5, importance=TRUE)
yhat.rf = predict(randomForestCarseats ,newdata=Carseats[- train ,])
# calculate MSE
mean((yhat.rf-carseats.test)^2)
# get importance of variables and plot
importance(randomForestCarseats)
varImpPlot(randomForestCarseats)

# MSE is 2.714168
```
The MSE is 2.960559 when m =3.
The MSE is 2.787584 when m =4.
The MSE is 2.714168 when m =5.

The number of variables considered at each split has an effect because starting with m=3 the error rate increased from 2.6 to 2.9 which is not ideal.Price, Shelveloc, and CompPrice are still the most important variables. As 
m becomes larger the error becomes smaller. The model was better off using bagging then a regular random forest with m=3, m=4, or m=5 because bagging produced a smaller error rate. 

# Chapter 8 Question 11

## This question uses the Caravan data set.

## a) Create a training set consisting of the first 1,000 observations,and a test set consisting of the remaining observations.
```{r,echo=FALSE,warning=FALSE}
rm(list=ls())
library(ISLR)
library(gbm)
Caravan$Purchase<-ifelse(Caravan$Purchase == "Yes",1,0)
# split data into training and test
caravantrain = Caravan[1:1000,]
caravantest = Caravan[1001:nrow(Caravan),]
print("train")
dim(caravantrain)
print("test")
dim(caravantest)
```

## b)Fit a boosting model to the training set with Purchase as the response and the other variables as predictors. Use 1,000 trees, and a shrinkage value of 0.01. Which predictors appear to be the most important?
```{r,echo=FALSE,warning = FALSE}
set.seed(1)
# boosting model
boost.caravan=gbm(Purchase~.,data=caravantrain, distribution = "bernoulli",n.trees =1000, interaction.depth =1, shrinkage =0.01,verbose=F)
summary(boost.caravan)

```
PPERSAUT is the most important predictor followed by MKOOPKLA and MOPLHOOG

## c) Use the boosting model to predict the response on the test data.Predict that a person will make a purchase if the estimated probability of purchase is greater than 20 %. Form a confusion matrix. What fraction of the people predicted to make a purchase do in fact make one? How does this compare with the results obtained from applying KNN or logistic regression to this data set?
```{r,echo=FALSE,warning = FALSE}
print("Boosting Model for Prediction Table")
yhat=predict(boost.caravan,newdata =caravantest,n.trees=1000,type = "response")
predictiontest <- ifelse(yhat > 0.2, 1, 0)
table(caravantest$Purchase, predictiontest)

print("Applying Logistic Regression for Prediction Table")
glm.fits = glm(Purchase~.,data=caravantrain,family=binomial)
yhat2=predict(glm.fits,newdata =caravantest,type = "response")
predictiontest <- ifelse(yhat2 > 0.2, 1, 0)
table(caravantest$Purchase, predictiontest)

```

With the boosting model,out of the 156 people predicted to make a purchase only 33 actually do which is just over 21%. Compared to the logistic regression model out of the 408 people predicted to make a purchase only 58 actually do which is roughly 14%.

# Exam Problem 1: Beauty Pays

## 1) Using the data, estimate the effect of “beauty” into course ratings. Make sure to think about the potential many “other determinants”. Describe your analysis and your conclusions.

```{r,echo = FALSE,warning=FALSE}
rm(list=ls())
beautyData<-read.csv("BeautyData.csv")
### Multiple Regression Model ###
linearModel <- lm(CourseEvals ~., data=beautyData)
summary(linearModel)
### Simple Linear Regression Model with just beauty predicting course evals ###
linearModel2 <- lm(CourseEvals ~ BeautyScore,data=beautyData)
summary(linearModel2)
cor(beautyData$CourseEvals,beautyData$female)
cor(beautyData$CourseEvals,beautyData$tenuretrack)
cor(beautyData$CourseEvals,beautyData$lower)
```
The BeautyScore coefficient from running a singular regression model with just BeautyScore predicting CourseEval and the BeautyScore coefficient from running a multiple linear regression model with all the variables both show that there is a statistically significant positive relationship between BeautyScore and CourseRating. The multiple regression model shows that holding all other variables constant, for each 1 point increase in the BeautyScore, the CourseRating is increased by 0.30415.The simple linear regression model with just BeautyScore shows similar results with the BeautyScore increases the CourseRating by 0.27148 for every 1 unit increase. The coefficients from the multiple regression equation also tell us that there is a negative correlation between being a female instructor, a lower instructor, a nonenglish speaking instructor, and an instructor on a tenuretrack and course ratings. The t-value and p-value significance level are much lower for tenure track meaning that his determinant may not be as powerful of a predictor for CourseRating as the other predictors but it is still a significant predictor in the model.

## 2) In his paper, Dr. Hamermesh has the following sentence: “Disentangling whether this outcome represents productivity or discrimination is, as with the issue generally, probably impossible”. Using the concepts we have talked about so far, what does he mean by that?
Dr.Hamermesh means that just because we can identify statistical significance and predict how beauty is going to effect to course evaluations does not mean that we can make a definitive statement about cause of the conclusions. Correlation does not equal causation. It is likely impossible for us to say if the outcome stems from productivity or discrimination because all we are looking at is the data that has been produced.

# Exam Problem 2: Housing Price Structure

## 1) Is there a premium for brick houses everything else being equal?
```{r, echo=FALSE,warning=FALSE}
rm(list=ls())
cityData <- read.csv("MidCity.csv")
BrickYes <- ifelse(cityData$Brick == "Yes",1,0)
BrickData <- data.frame(Home=cityData$Home,Nbhd=cityData$Nbhd, Offers=cityData$Offers,SqFt=cityData$SqFt,BrickYes=BrickYes,Bedrooms=cityData$Bedrooms,Bathrooms=cityData$Bathrooms,Price=cityData$Price)
linearModel <- lm(Price ~Nbhd+Offers+SqFt+BrickYes+Bedrooms+Bathrooms, data=BrickData)
summary(linearModel)

```
The multiple regression model shows that there is a premium for brick houses all else being equal.Brick is a boolean data type, in order to filter out the homes in the data that are indicated as being Brick. I created a variable called "Brick Yes" that converted the Brick = "Yes" elements to the number 1 and the Brick = "No" elements to the number 0.The Brick Yes predictor is statistically significant with a p-value below 0.05 and a high t-value. The model shows that a brick house increases the home price by 15603.19 dollars on average. 


## 2) Is there a premium for houses in neighborhood 3?

```{r,echo=FALSE,warning=FALSE}
BrickYes <- ifelse(cityData$Brick == "Yes",1,0)
N2 <-ifelse(cityData$Nbhd == 2,1,0)
N3 <- ifelse(cityData$Nbhd == 3,1,0)
BrickData <- data.frame(Home=cityData$Home,N2=N2,N3=N3, Offers=cityData$Offers,SqFt=cityData$SqFt,BrickYes=BrickYes,Bedrooms=cityData$Bedrooms,Bathrooms=cityData$Bathrooms,Price=cityData$Price)
linearModel <- lm(Price ~N2+N3+Offers+SqFt+BrickYes+Bedrooms+Bathrooms, data=BrickData)
summary(linearModel)
```

I decided to break the neighborhoods up into their own columns of the dataframe to ensure that the coefficient produced for N3 was independent of the other neighborhoods. I had to convert the N2 and N3 elements to either 1 or 0 to keep the values as boolean to check for the predictor effect in the model. This was not necessary for N1 because the value would already be 1 if the home was in N1. N3 was a boolean data type that I updated to 1 or 0 based on if the value was a 3 in the original data set. The N3 predictor is a statistically significant value to predict price based on the p-value below 0.05 and the high t-value. The model shows that a home in neighborhood 3 increases the price by an average of 20681.03 dollars.

## 3) Is there an extra premium for brick houses in neighborhood 3?
```{r,echo=FALSE,warning=FALSE}
BrickYes <- ifelse(cityData$Brick == "Yes",1,0)
N2 <-ifelse(cityData$Nbhd == 2,1,0)
N3 <- ifelse(cityData$Nbhd == 3 ,1,0)
N3BrickYes <- ifelse(cityData$Brick == "Yes",ifelse(cityData$Nbhd== 3,1,0),0)
BrickData <- data.frame(Home=cityData$Home,N3BrickYes = N3BrickYes, N2=N2,N3=N3,Offers=cityData$Offers,SqFt=cityData$SqFt,BrickYes=BrickYes,Bedrooms=cityData$Bedrooms,Bathrooms=cityData$Bathrooms,Price=cityData$Price)
linearModel <- lm(Price ~ N3BrickYes + N2+ N3 + Offers + SqFt + BrickYes + Bedrooms + Bathrooms, data=BrickData)
summary(linearModel)

```
I created a new predictor called N3BrickYes that combines the conditions of BrickYes and N3. The model shows that N3BrickYes is a statistically significant predictor with a p-value below 0.05 we can conclude that a brick house in neighborhood 3 increases the price of a home by 10181.57 dollars on average and we can conclude that there is an extra premium for brick houses in neighborhood 3.

## 4) For the purposes of prediction could you combine the neighborhoods 1 and 2 into a single “older” neighborhood?
```{r,echo=FALSE,warning=FALSE}
BrickYes <- ifelse(cityData$Brick == "Yes",1,0)
N3 <- ifelse(cityData$Nbhd == 3 ,0,1)
BrickData <- data.frame(Home=cityData$Home, N1andN2=N3,Offers=cityData$Offers,SqFt=cityData$SqFt,BrickYes=BrickYes,Bedrooms=cityData$Bedrooms,Bathrooms=cityData$Bathrooms,Price=cityData$Price)
linearModel <- lm(Price ~ N1andN2 + Offers + SqFt + BrickYes + Bedrooms + Bathrooms, data=BrickData)
summary(linearModel)
```
I combined the N1 and N2 homes by using the N3 column and assigning a 0 to all the elements with "Yes" for N3 and all the elements with "No" I assigned a 1, allowing all of the 1's to be N1 and N2 homes. The model indicates that N1 and N2 grouped together into an older neighborhood is a statistically significant predictor with a high t-value and p-value below 0.05. The model indicates that a home in the older neighborhood of N1 or N2 decreases the price by 21937.57 dollars on average.

# Exam Question 3:What causes what?

## 1) Why can’t I just get data from a few different cities and run the regression of “Crime” on “Police” to understand how more cops in the streets affect crime? (“Crime” refers to some measure of crime rate and “Police” measures the number of cops in a city)
You can't run a regression of crime on police to understand how more cops in the streets affect crime because you won't be able to tell if more police lead to more crime or more crime leads to more police. This is because high crime cities have an incentive to hire more police thus skewing any causation analysis

## 2) How were the researchers from UPENN able to isolate this effect? Briefly describe their approach and discuss their result in the “Table 2” below.
The researchers from UPENN were able to isolate this event by studying the crime rate in Washington,DC when the terror threat level is increased to orange. By law when the terror threat level is increased to orange, there is an increase in the police force in the city. This allowed the researchers to study if on a day where there is increased police presence for a reason unrelated to street crime, is there an increase or decrease in street crime. This scenario helped researchers isolate the causal relationship between crime and police presence. The research found that crime did in fact go down on the days of increased police presence due to the high terror alert. The researchers also controlled for METRO ridership to ensure that people potentially staying home out of fear, and thus creating a smaller pool of potential crime victims, would not skew the model testing for higher police presence effect on crime rates. 

The result of the study showed that when not taking into account METRO ridership, the increase in police presence on the days of the high terror alert, decreased the crime rate by -7.316 per day on average. This higher police force metric is also statistically significant for predicting the crime rate because the p-value was below 0.05. When controlling for METRO ridership the high police presence decreased the crime rate by -6.046 per day on average and the METRO ridership control value actually showed a positive coefficient at 17.341 that is statistically significant at the 0.01 p-value level. The results of the study show that increasing police presence can have a negative on street crime.

## 3)Why did they have to control for METRO ridership? What was that trying to capture?
The researchers had to control for METRO ridership because if more people in general are staying home because they are scared, that would mean less people on the streets, and thus less opportunity and victims for street crime. In order to control for this the researchers created a model that took METRO ridership into consideration. The control for METRO ridership was trying to ensure that even while controlling for METRO ridership, the high police presence lead to a decrease in crime. The control was to ensure that no external factors, such as people staying home out of fear, would skew the model results. 

## 4) In the next page, I am showing you “Table 4” from the research paper. Just focus on the first column of the table. Can you describe the model being estimated here? What is the conclusion?

The model being estimated in Table 4 is trying to predict the effect of location and high police presence on crime rate. The title of the table is "Reduction in Crime on High-Alert Days: Concentration on the National Mall" meaning that the model is trying to focus on the effect of high-alert police presence and the location of the National Mall, on the rate of crime. The National Mall is likely in police District 1 and that is why the first coefficient is high police alert and District 1 and the second coefficient is high police alert and all the rest of the districts. The model is trying to isolate the effect of including District 1's location in the crime rate. The National Mall being located in District 1 would mean that on a day with a high terror threat, there would be an even greater presence of police in the National Mall area because it is a terror target. The table results show that by including location in this model, only the coefficient of high-police and District 1 has a statistically significant coefficient. This means that while we can conclude that an increased police presence in District 1 leads to a lower crime rate by -2.621 on average, we cannot conclude that the a higher police presence leads to lower crime in any of the other districts because the coefficient is not statistically significant. The control value for METRO ridership is also statistically significant in this model and the other control value. 

# Exam Question 4: Neural Nets

## Re-run the Boston housing data example using a single layer neural net. Cross validate for a few choices of Size and decay parameters.

```{r,echo=FALSE}
rm(list=ls())
library(MASS)
attach(Boston)


###standardize the x's
minv = rep(0,13)
maxv = rep(0,33)
zagsc = Boston
for(i in 1:13) {
  minv[i] = min(Boston[[i]])
  maxv[i] = max(Boston[[i]])
  zagsc[[i]] = (Boston[[i]]-minv[i])/(maxv[i]-minv[i])
}

library(nnet)
library(NeuralNetTools)

set.seed(1)
znn = nnet(crim~medv,zagsc,size=3,decay=.1,linout=T)


###get fits, print summary,  and plot fit
fznn = predict(znn,zagsc)
plot(zagsc$medv,zagsc$crim)
oo = order(zagsc$medv)
lines(zagsc$medv[oo],fznn[oo],col="red",lwd=2)
abline(lm(crim~medv,zagsc)$coef)

summary(znn)
NeuralNetTools::plotnet(znn)

### How does it work
## Zagsc fit from model using size 3 and decay .1
print(summary(znn))
x = zagsc$medv
y = zagsc$crim

z1 = 0.34 -0.18 *x
z2 = -0.01 +0.03*x
z3 = -0.18 +0.17*x

f1 = 0.75*exp(z1)/(1+exp(z1))
f2 = 0.23*exp(z2)/(1+exp(z2))
f3 = -0.42*exp(z3)/(1+exp(z3))

oo = order(x)
plot(x,y+0.26)
lines(x[oo],f1[oo],col=2) 
lines(x[oo],f2[oo],col=3) 
lines(x[oo],f3[oo],col=4) 
lines(x[oo],(f1+f2+f3)[oo],col=5)

### setting up graph and data frame for the cross validation for loop

set.seed(23)
x = runif(1000)
x = sort(x)
y = exp(-80*(x-.5)*(x-.5)) + .05*rnorm(1000)
plot(x,y)
df = data.frame(y=y,x=x)

for(i in 1:10) {
  for(s in 1:5)
  nnsim = nnet(y~x,df,size=s,decay = 1/2^i,linout=T,maxit=1000)
  simfit = predict(nnsim,df)
  lines(x,simfit,col=i,lwd=3)
  print(i)
  print(s)
  readline()
}

print("cross validation loop shows that fit is best at size 5 and decay 10")

set.seed(99)
nnsim = nnet(y~x,df,size=5,decay=1/2^10,linout=T,maxit=1000)
thefit = predict(nnsim,df)
NeuralNetTools::plotnet(nnsim)
plot(x,y)
lines(x,thefit,col="blue",lwd=3,cex.axis=1.5,cex.lab=1.5)



z1 = -3.62 +6.22*x
z2 = -6.49 +13.03*x
z3 = -8.23 +18.41*x
z4 = 0.04 - 0.26*x
z5 = -2.32 +2.38*x

F = function(x) {return(exp(x)/(1+exp(x)))}
f1 = 3.48*F(z1)
f2 = -7.30*F(z2)
f3 = 5.34*F(z3)
f4 = 0.408*F(z4)
f5 = -2.81*F(z5)

rx=range(x)
ry = range(c(f1,f2,f3,f4,f5,y))
plot(rx,ry,type="n",xlab="x",ylab="fit",cex.axis=2,cex.lab=2)
points(x,y)
lines(x,f1,col=1,lwd=2)
lines(x,f2,col=2,lwd=2)
lines(x,f3,col=3,lwd=2)
lines(x,f4,col=4,lwd=2)
lines(x,f5,col=5,lwd=2)
lines(x,-0.02+f1+f2+f3+f4+f5,col=6,lwd=4)


```
I chose median home value as the sole predictor for Boston crime rates.I then used a cross-validation for loop to select different values for size and decay and found that a size of 5 and decay of 10 created the best fit on the neural net. 


# Exam Question 5: Final Project 

## Describe your contribution to the final group project 
I contributed to the final project first by doing extensive research to suggest three different data sets to use. This initial research was helpful to allow my group to discuss all the potential data sets we could use and the positives and negatives to using a specific data set vs another one.

Additionally I contributed to the final project by initially running several models for our data including a boosting, lasso, random forest, ridge regression, multiple linear regression, and a K Nearest neighbors model. Several of my group members also ran similar models and we compared them to chose the final models for the presentation. 

In addition to the modeling I also contributed to my group's final presentation. I created the initial PowerPoint slide deck outline. I also populated the slides for my models. During the presentation I presented the linear models for my group.

Our group worked really well together and we all contributed models and slides to the PowerPoint presentation. Our group collaborated really well and all group members contributed to the presentation. 
